{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install -r requirement.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LdmMqI-L3FCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TfWojTZ27Qp"
      },
      "outputs": [],
      "source": [
        "import streamlit as st"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n"
      ],
      "metadata": {
        "id": "5ZiT8U5C5WKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "if 'OPENAI_API_KEY' in os.environ:\n",
        "    print(\"OPENAI_API_KEY is set.\")\n",
        "else:\n",
        "    print(\"OPENAI_API_KEY is not set.\")\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.set_page_config(page_title=\"Chat with your Document\", layout=\"wide\")\n",
        "st.title(\"üìÑ Chat with your Document\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload a document\", type=[\"pdf\", \"txt\", \"docx\"])\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = []\n",
        "if \"qa_chain\" not in st.session_state:\n",
        "    st.session_state.qa_chain = None\n",
        "\n",
        "if uploaded_file is not None and st.session_state.qa_chain is None:\n",
        "    # --- Load Document ---\n",
        "    file_type = uploaded_file.name.split(\".\")[-1].lower()\n",
        "    with open(\"temp.\" + file_type, \"wb\") as f:\n",
        "        f.write(uploaded_file.getbuffer())\n",
        "\n",
        "    if file_type == \"pdf\":\n",
        "        loader = PyPDFLoader(\"temp.pdf\")\n",
        "    elif file_type == \"txt\":\n",
        "        loader = TextLoader(\"temp.txt\")\n",
        "    elif file_type == \"docx\":\n",
        "        loader = Docx2txtLoader(\"temp.docx\")\n",
        "    else:\n",
        "        st.error(\"Unsupported file type\")\n",
        "        st.stop()\n",
        "\n",
        "    documents = loader.load()\n",
        "\n",
        "    # --- Split & Embed ---\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = splitter.split_documents(documents)\n",
        "\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vectordb = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "    # --- Conversational Chain ---\n",
        "    llm = ChatOpenAI(temperature=0)\n",
        "    st.session_state.qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm, vectordb.as_retriever(), return_source_documents=True\n",
        "    )\n",
        "    st.success(\"‚úÖ Document processed! You can start chatting below.\")\n",
        "    print('Documetnt Processed')\n",
        "    print('vectordb : ', vectordb)\n",
        "    print('qa_chain : ', st.session_state.qa_chain)\n",
        "\n",
        "# --- Chat Interface ---\n",
        "if st.session_state.qa_chain:\n",
        "    print('Inside the qa_chain')\n",
        "    query = st.text_input(\"Ask a question about your document:\")\n",
        "    if query:\n",
        "        result = st.session_state.qa_chain({\"question\": query, \"chat_history\": st.session_state.chat_history})\n",
        "        answer = result[\"answer\"]\n",
        "        st.session_state.chat_history.append((query, answer))\n",
        "\n",
        "        # Display conversation\n",
        "        for q, a in st.session_state.chat_history:\n",
        "            st.markdown(f\"**You:** {q}\")\n",
        "            st.markdown(f\"**Bot:** {a}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bYqU1zc2-_Z",
        "outputId": "25e7dc0e-33b5-4c83-a1ec-ee5ea100b90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVlqg45n7gxy",
        "outputId": "a471ebd8-7ebf-4187-e6a1-1ebba2caabce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.55.221.252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB9attJG7pwS",
        "outputId": "cf4d4d3b-ad5d-48b8-d3ea-1aa410d98148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.55.221.252:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0Kyour url is: https://clever-clouds-hear.loca.lt\n",
            "OPENAI_API_KEY is set.\n",
            "OPENAI_API_KEY is set.\n",
            "OPENAI_API_KEY is set.\n",
            "Documetnt Processed\n",
            "vectordb :  <langchain_community.vectorstores.faiss.FAISS object at 0x7de3f0b89970>\n",
            "qa_chain :  verbose=False combine_docs_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7de3f0b89c40>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7de3f0b8a720>, root_client=<openai.OpenAI object at 0x7de3f1012840>, root_async_client=<openai.AsyncOpenAI object at 0x7de3f0b8a030>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') question_generator=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7de3f0b89c40>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7de3f0b8a720>, root_client=<openai.OpenAI object at 0x7de3f1012840>, root_async_client=<openai.AsyncOpenAI object at 0x7de3f0b8a030>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}) return_source_documents=True retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7de3f0b89970>, search_kwargs={})\n",
            "Inside the qa_chain\n",
            "OPENAI_API_KEY is set.\n",
            "OPENAI_API_KEY is set.\n",
            "Documetnt Processed\n",
            "vectordb :  <langchain_community.vectorstores.faiss.FAISS object at 0x7de3f103d670>\n",
            "qa_chain :  verbose=False combine_docs_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7de3f11d2690>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7de3f0b8a4b0>, root_client=<openai.OpenAI object at 0x7de3f1932f90>, root_async_client=<openai.AsyncOpenAI object at 0x7de3f17893d0>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') question_generator=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7de3f11d2690>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7de3f0b8a4b0>, root_client=<openai.OpenAI object at 0x7de3f1932f90>, root_async_client=<openai.AsyncOpenAI object at 0x7de3f17893d0>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}) return_source_documents=True retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7de3f103d670>, search_kwargs={})\n",
            "Inside the qa_chain\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import streamlit as st\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory  # memory\n",
        "\n",
        "\n",
        "# --- API Key ---\n",
        "os.environ[\"OPENAI_API_KEY\"] = '##'\n",
        "\n",
        "if 'OPENAI_API_KEY' in os.environ:\n",
        "    print(\"OPENAI_API_KEY is set.\")\n",
        "else:\n",
        "    print(\"OPENAI_API_KEY is not set.\")\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.set_page_config(page_title=\"Chat with your Document\", layout=\"wide\")\n",
        "st.title(\"üìÑ Chat with your Document\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload a document\", type=[\"pdf\", \"txt\", \"docx\"])\n",
        "if \"qa_chain\" not in st.session_state:\n",
        "    st.session_state.qa_chain = None\n",
        "\n",
        "if uploaded_file is not None and st.session_state.qa_chain is None:\n",
        "    # --- Load Document ---\n",
        "    file_type = uploaded_file.name.split(\".\")[-1].lower()\n",
        "    with open(\"temp.\" + file_type, \"wb\") as f:\n",
        "        f.write(uploaded_file.getbuffer())\n",
        "\n",
        "    if file_type == \"pdf\":\n",
        "        loader = PyPDFLoader(\"temp.pdf\")\n",
        "    elif file_type == \"txt\":\n",
        "        loader = TextLoader(\"temp.txt\")\n",
        "    elif file_type == \"docx\":\n",
        "        loader = Docx2txtLoader(\"temp.docx\")\n",
        "    else:\n",
        "        st.error(\"Unsupported file type\")\n",
        "        st.stop()\n",
        "\n",
        "    documents = loader.load()\n",
        "\n",
        "    # --- Split & Embed ---\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = splitter.split_documents(documents)\n",
        "\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    vectordb = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "    # --- Conversational Chain with Memory ---\n",
        "    llm = ChatOpenAI(temperature=0)\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True,\n",
        "        output_key=\"answer\"   # üëà tell memory to use \"answer\"\n",
        "    )\n",
        "\n",
        "    st.session_state.qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm,\n",
        "        retriever=vectordb.as_retriever(),\n",
        "        memory=memory,\n",
        "        return_source_documents=True,\n",
        "        output_key=\"answer\"   # üëà also set here\n",
        "    )\n",
        "\n",
        "    st.success(\"‚úÖ Document processed! You can start chatting below.\")\n",
        "\n",
        "# --- Chat Interface ---\n",
        "if st.session_state.qa_chain:\n",
        "    query = st.text_input(\"Ask a question about your document:\")\n",
        "    if query:\n",
        "        result = st.session_state.qa_chain({\"question\": query})\n",
        "        answer = result[\"answer\"]\n",
        "\n",
        "        # Display chat history from memory\n",
        "        for msg in result[\"chat_history\"]:\n",
        "            role = \"You\" if msg.type == \"human\" else \"Bot\"\n",
        "            st.markdown(f\"**{role}:** {msg.content}\")\n"
      ],
      "metadata": {
        "id": "4uRn-MSu7wLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!"
      ],
      "metadata": {
        "id": "aEcLfez_8Gzt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}