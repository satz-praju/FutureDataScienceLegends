{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IXUiF2GTpM5Y"
      },
      "outputs": [],
      "source": [
        "#pip install -r requirements.txt\n",
        "!pip install --upgrade langchain langchain-core\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')"
      ],
      "metadata": {
        "id": "xUIzC0CYrOBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory,\n",
        "from langchain.schema import SystemMessage\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "vxJdqaEFrXP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "zdpTd38RsSLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = FileChatMessageHistory('chat_history.json')\n",
        "memory = ConversationBufferMemory(#memory_key=\"chat_history\",  #in-memory\n",
        "                                  memory_key=\"chat_history\", #in Hard disk\n",
        "                                  chat_memory=history,\n",
        "                                  return_messages=True)"
      ],
      "metadata": {
        "id": "IVo4oqGkt1Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessage(content = \"You are a chatbot having a conversation with a human. Always respond in a polite manner. Always respond with a emojis. Write the answer in a tabular format and in bullet points\"),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "3JFtMl39uS3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm = llm,\n",
        "         prompt = prompt,\n",
        "         memory = memory,\n",
        "         verbose = False)"
      ],
      "metadata": {
        "id": "jcVHnxX1vBA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  content = input(\"Your Prompt: \")\n",
        "  if content == \"exit\":\n",
        "    break\n",
        "  else:\n",
        "    response = chain.invoke({'content': content})\n",
        "    print(response['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYCrw101vn_9",
        "outputId": "267c4070-06ef-4904-d8c4-4b932bc41715"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your Prompt: do you know where i m from?\n",
            "| ðŸ˜Š |\n",
            "|---|\n",
            "| Yes, you mentioned you are from Dubai! It's a beautiful city! |\n",
            "\n",
            "- Yes, you mentioned you are from Dubai! It's a beautiful city!\n",
            "Your Prompt: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoqMF1rBw_ud",
        "outputId": "be6617c0-2623-4c7b-a78e-42e0defc679e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='do you know my name?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"| ðŸ˜Š |\\n|---|\\n| I'm sorry, but I don't have the ability to know your name. But, I'd be happy to call you whatever you'd like! |\\n\\n- I'm sorry, I don't have the ability to know your name. But I'd be happy to call you whatever you'd like!\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='MY name is Noor, I am from dubai', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"| ðŸ˜Š |\\n|---|\\n| Hello Noor! It's nice to meet you from Dubai! How can I assist you today? |\\n\\n- Hello Noor! It's nice to meet you from Dubai! How can I assist you today?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='do you know where i m from?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"| ðŸ˜Š |\\n|---|\\n| Yes, you mentioned you are from Dubai! It's a beautiful city! |\\n\\n- Yes, you mentioned you are from Dubai! It's a beautiful city!\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yCOZe0lfzw2Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}