{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe0e504-4ef5-4e02-838b-56d5fa560f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = 'sk-proj-*******'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54201e9d-05f0-4c7b-be3e-bff00563e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai\n",
    "langchain\n",
    "langchain_openai\n",
    "langchain_experimental\n",
    "langchainhub\n",
    "pinecone-client\n",
    "python-dotenv\n",
    "tiktoken\n",
    "docx2txt\n",
    "pypdf\n",
    "requests\n",
    "numpy\n",
    "pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93fa9db-5391-4895-87d3-f9655c9d76a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/homebrew/lib/python3.11/site-packages (from -r requirement.txt (line 1)) (1.33.0)\n",
      "Requirement already satisfied: langchain in /opt/homebrew/lib/python3.11/site-packages (from -r requirement.txt (line 2)) (0.1.0)\n",
      "Collecting langchain_openai (from -r requirement.txt (line 3))\n",
      "  Downloading langchain_openai-0.1.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain_experimental (from -r requirement.txt (line 4))\n",
      "  Downloading langchain_experimental-0.0.61-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchainhub (from -r requirement.txt (line 5))\n",
      "  Downloading langchainhub-0.1.20-py3-none-any.whl.metadata (659 bytes)\n",
      "Collecting pinecone-client (from -r requirement.txt (line 6))\n",
      "  Downloading pinecone_client-4.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: python-dotenv in /opt/homebrew/lib/python3.11/site-packages (from -r requirement.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: tiktoken in /opt/homebrew/lib/python3.11/site-packages (from -r requirement.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: docx2txt in /opt/homebrew/lib/python3.11/site-packages (from -r requirement.txt (line 9)) (0.8)\n",
      "Requirement already satisfied: pypdf in /opt/homebrew/lib/python3.11/site-packages (from -r requirement.txt (line 10)) (3.17.4)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from -r requirement.txt (line 11)) (2.31.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from -r requirement.txt (line 12)) (1.25.0)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from -r requirement.txt (line 13)) (2.1.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/lib/python3.11/site-packages (from openai->-r requirement.txt (line 1)) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.11/site-packages (from openai->-r requirement.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from openai->-r requirement.txt (line 1)) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/lib/python3.11/site-packages (from openai->-r requirement.txt (line 1)) (2.5.2)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.11/site-packages (from openai->-r requirement.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/lib/python3.11/site-packages (from openai->-r requirement.txt (line 1)) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/homebrew/lib/python3.11/site-packages (from openai->-r requirement.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/homebrew/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in /opt/homebrew/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (0.0.10)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in /opt/homebrew/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (0.1.8)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /opt/homebrew/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (0.0.77)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain->-r requirement.txt (line 2)) (8.2.3)\n",
      "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain_openai (from -r requirement.txt (line 3))\n",
      "  Downloading langchain_openai-0.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.7 (from langchain->-r requirement.txt (line 2))\n",
      "  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-experimental to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain_experimental (from -r requirement.txt (line 4))\n",
      "  Downloading langchain_experimental-0.0.60-py3-none-any.whl.metadata (2.1 kB)\n",
      "  Downloading langchain_experimental-0.0.59-py3-none-any.whl.metadata (2.1 kB)\n",
      "  Downloading langchain_experimental-0.0.58-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langchain (from -r requirement.txt (line 2))\n",
      "  Downloading langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.38 (from langchain->-r requirement.txt (line 2))\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain->-r requirement.txt (line 2))\n",
      "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain->-r requirement.txt (line 2))\n",
      "  Downloading langsmith-0.1.81-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchainhub->-r requirement.txt (line 5))\n",
      "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub->-r requirement.txt (line 5))\n",
      "  Downloading types_requests-2.32.0.20240622-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /opt/homebrew/lib/python3.11/site-packages (from pinecone-client->-r requirement.txt (line 6)) (2023.7.22)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client->-r requirement.txt (line 6))\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/homebrew/lib/python3.11/site-packages (from pinecone-client->-r requirement.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/lib/python3.11/site-packages (from tiktoken->-r requirement.txt (line 8)) (2023.12.25)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->-r requirement.txt (line 11)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->-r requirement.txt (line 11)) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/m.alaudeen/Library/Python/3.11/lib/python/site-packages (from pandas->-r requirement.txt (line 13)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->-r requirement.txt (line 13)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->-r requirement.txt (line 13)) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirement.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirement.txt (line 2)) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirement.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r requirement.txt (line 1)) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirement.txt (line 1)) (0.14.0)\n",
      "Collecting packaging<25,>=23.2 (from langchainhub->-r requirement.txt (line 5))\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain->-r requirement.txt (line 2))\n",
      "  Downloading orjson-3.10.5-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r requirement.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r requirement.txt (line 1)) (2.14.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/m.alaudeen/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->-r requirement.txt (line 13)) (1.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain->-r requirement.txt (line 2)) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->-r requirement.txt (line 2)) (1.0.0)\n",
      "Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
      "Downloading langchain_experimental-0.0.58-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchainhub-0.1.20-py3-none-any.whl (5.0 kB)\n",
      "Downloading pinecone_client-4.1.1-py3-none-any.whl (216 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.2/216.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
      "Downloading langsmith-0.1.81-py3-none-any.whl (127 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.1/127.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Downloading types_requests-2.32.0.20240622-py3-none-any.whl (15 kB)\n",
      "Downloading orjson-3.10.5-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (258 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.7/258.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: types-requests, pinecone-plugin-interface, packaging, orjson, pinecone-client, langchainhub, langsmith, langchain-core, langchain-text-splitters, langchain_openai, langchain-community, langchain, langchain_experimental\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: orjson\n",
      "    Found existing installation: orjson 3.8.14\n",
      "    Uninstalling orjson-3.8.14:\n",
      "      Successfully uninstalled orjson-3.8.14\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.77\n",
      "    Uninstalling langsmith-0.0.77:\n",
      "      Successfully uninstalled langsmith-0.0.77\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.8\n",
      "    Uninstalling langchain-core-0.1.8:\n",
      "      Successfully uninstalled langchain-core-0.1.8\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.0.10\n",
      "    Uninstalling langchain-community-0.0.10:\n",
      "      Successfully uninstalled langchain-community-0.0.10\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.1.0\n",
      "    Uninstalling langchain-0.1.0:\n",
      "      Successfully uninstalled langchain-0.1.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mlserver 1.3.5 requires fastapi!=0.89.0,<=0.89.1,>=0.88.0, but you have fastapi 0.105.0 which is incompatible.\n",
      "poetry 1.7.1 requires platformdirs<4.0.0,>=3.0.0, but you have platformdirs 4.1.0 which is incompatible.\n",
      "datasets 2.16.1 requires fsspec[http]<=2023.10.0,>=2023.1.0, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-0.1.20 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.2 langchain_experimental-0.0.58 langchain_openai-0.1.7 langchainhub-0.1.20 langsmith-0.1.81 orjson-3.10.5 packaging-23.2 pinecone-client-4.1.1 pinecone-plugin-interface-0.0.7 types-requests-2.32.0.20240622\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1dc115-a23c-40c9-b3de-3cbd12b8312f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 1.33.0\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: \n",
      "Location: /opt/homebrew/lib/python3.11/site-packages\n",
      "Requires: anyio, distro, httpx, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: guidance, langchain-openai, llama-index\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f403b098-c9ed-4243-a1bf-7ab4883edd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.1.20\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /opt/homebrew/lib/python3.11/site-packages\n",
      "Requires: aiohttp, dataclasses-json, langchain-community, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: langchain-experimental\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341107c6-5242-4269-b67b-d09288312675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a689f2-c33f-4cc6-8774-b4c474613a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv(), override=True)\n",
    "key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a06ba9d-a286-4ac3-9c2d-b550e5bcf075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-proj-************************'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98a97465-961f-41a8-ad18-99cde67ee34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Newton's third law of motion states that for every action, there is an equal and opposite reaction.\", role='assistant', function_call=None, tool_calls=None)\n",
      "Newton's third law of motion states that for every action, there is an equal and opposite reaction.\n"
     ]
    }
   ],
   "source": [
    "### OPENAI FRAMEWORK\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-16k\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain the Newtons third law of motion in one sentence!\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba97472e-8467-4804-af6a-93a3a2bd6d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='As of October 2021, the Chief Minister of Andhra Pradesh is Y.S. Jaganmohan Reddy.', role='assistant', function_call=None, tool_calls=None)\n",
      "As of October 2021, the Chief Minister of Andhra Pradesh is Y.S. Jaganmohan Reddy.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-16k\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"who is the cheif minister of andhra pradesh?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23d833cf-b640-4947-aab2-93666e8d585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "473f6d8a-513c-4804-9a13-b8067d73c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d68a842-83aa-4b86-9d4f-4253443ad8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.invoke(\"Explain the Newtons third law of motion in one sentence\", model = 'gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ede8ffc-9ad0-4994-a5b0-2be7d1a597ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For every action, there is an equal and opposite reaction.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4591ff14-90d8-4d92-bca4-0036b633d279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16cf977b-ee4f-4608-9be1-abf8e58f7a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43cbae1e-d80a-49d1-b047-d520ec064be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [SystemMessage(content = 'You are a great, funny, humourous physics teacher. you always try to explain things very simply like teaching it to 5th Grade Student. You always giv e the response in simple bullet points or in tabular format based on the question asked. you always answer in one single sentence'),\n",
    "           HumanMessage(content = 'Explain the quantum physics?')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ac9b5f4-8cf0-41c1-8528-f9ad0bde6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba5a4e64-b711-48a9-8a2f-cd6cfa7fe5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Quantum physics is the study of very small particles like atoms and subatomic particles.\\n- It helps us understand how these tiny particles behave and interact with each other.\\n- Quantum physics is different from classical physics because it deals with probabilities and uncertainties at the microscopic level.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfe4c1a9-9f94-4c3d-875e-335792c1e660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'LanguageModelInput'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[RunnableConfig]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[List[str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'BaseMessage'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Transform a single input into an output. Override to implement.\n",
       "\n",
       "Args:\n",
       "    input: The input to the runnable.\n",
       "    config: A config to use when invoking the runnable.\n",
       "       The config supports standard keys like 'tags', 'metadata' for tracing\n",
       "       purposes, 'max_concurrency' for controlling how much work to do\n",
       "       in parallel, and other keys. Please refer to the RunnableConfig\n",
       "       for more details.\n",
       "\n",
       "Returns:\n",
       "    The output of the runnable.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm.invoke?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fb8ce9d-a03d-4cb0-b07a-59d9df101fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='- Quantum physics is the study of very small particles like atoms and subatomic particles.\\n- It helps us understand how these tiny particles behave and interact with each other.\\n- Quantum physics is different from classical physics because it deals with probabilities and uncertainties at the microscopic level.', response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 76, 'total_tokens': 129}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1d33af02-0d82-4043-b888-6d8be86e8314-0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f53eb261-5827-4129-9dd2-d313980dd157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Noordeen! How can I assist you today?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [SystemMessage(content = 'You are a customer support Assistant'),\n",
    "           HumanMessage(content = 'My name is Noordeen')]\n",
    "output = llm.invoke(messages)\n",
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb1a19d2-b9db-4f85-9451-8000b882853b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to your personal information. How can I assist you today?\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [SystemMessage(content = 'You are a customer support Assistant'),\n",
    "           HumanMessage(content = 'What is my name?')]\n",
    "output = llm.invoke(messages)\n",
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188e61eb-8cb8-4ed9-b497-6ca290929720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cc740bc-c56c-431a-88a6-d3a9df86af07",
   "metadata": {},
   "source": [
    "# Caching LLM Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a080d349-0947-43a0-801e-528bceb13d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19c5c71f-e74b-4d32-a3d5-54c42207006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac9fdeff-4aa8-4847-ab73-cba5d5da704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0005e660-c193-4990-b0f7-60b0c8746196",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6cb7c46b-f8bb-4dbc-b0a8-89cafe5ef790",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me a poem where 5th grade student can understand?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "53d8b59c-09e5-47e4-8c18-245d161e725c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.94 ms, sys: 2.4 ms, total: 12.3 ms\n",
      "Wall time: 2.89 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nOh little one, with curious mind\\nCome and sit, and you will find\\nA world of words, ready to explore\\nIn this poem, there's so much more\\n\\nRhymes and rhythms, they make it fun\\nLike music for your eyes to run\\nAlong the lines, up and down\\nLet's go on an adventure, let's go to town\\n\\nIn this world, there are no rules\\nJust let your imagination be your tools\\nWith words, you can paint a scene\\nOf mountains high and valleys green\\n\\nYou can tell a story, make up a tale\\nWith each word, your imagination will prevail\\nSo don't be afraid, just let it flow\\nYour words will grow and grow and grow\\n\\nAnd when you're done, you'll see\\nA masterpiece, created by thee\\nSo go on, little one, and write\\nA poem that will shine so bright\\n\\nFor in this world of words and rhyme\\nThere's no limit, no mountain to climb\\nJust let your 5th grade mind be free\\nAnd see what wonders you can be.\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d2c75d6c-976d-4f9b-aaa5-3681bf869164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 395 µs, sys: 2 µs, total: 397 µs\n",
      "Wall time: 420 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nOh little one, with curious mind\\nCome and sit, and you will find\\nA world of words, ready to explore\\nIn this poem, there's so much more\\n\\nRhymes and rhythms, they make it fun\\nLike music for your eyes to run\\nAlong the lines, up and down\\nLet's go on an adventure, let's go to town\\n\\nIn this world, there are no rules\\nJust let your imagination be your tools\\nWith words, you can paint a scene\\nOf mountains high and valleys green\\n\\nYou can tell a story, make up a tale\\nWith each word, your imagination will prevail\\nSo don't be afraid, just let it flow\\nYour words will grow and grow and grow\\n\\nAnd when you're done, you'll see\\nA masterpiece, created by thee\\nSo go on, little one, and write\\nA poem that will shine so bright\\n\\nFor in this world of words and rhyme\\nThere's no limit, no mountain to climb\\nJust let your 5th grade mind be free\\nAnd see what wonders you can be.\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3e8beff6-d3c0-4f75-8dc2-7c7fe1622301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "825fdca8-a6d8-4ff5-a654-83609343d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell a joke ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "384eebf6-e63a-4bff-b733-200f2630e5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.14 ms, sys: 919 µs, total: 3.06 ms\n",
      "Wall time: 2.32 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "de155921-28ac-4820-8203-4b0c16534f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything.\""
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ed6818e1-a69b-4ea4-b54c-7f517e3a268a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.81 ms, sys: 1.52 ms, total: 4.34 ms\n",
      "Wall time: 3.34 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "10ed46be-c01a-4db2-9925-739b664f9aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired!\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b47fcb12-c837-49d3-8965-63bfaf6ab5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('.langchain.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b85a3085-a21f-4af2-a230-72238f165ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of tables\n",
      "\n",
      "[('full_llm_cache',), ('full_md5_llm_cache',)]\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\"SELECT name FROM sqlite_master \n",
    "    WHERE type='table';\"\"\"\n",
    " \n",
    "# Creating cursor object using connection object\n",
    "cursor = conn.cursor()\n",
    "     \n",
    "    # executing our sql query\n",
    "cursor.execute(sql_query)\n",
    "print(\"List of tables\\n\")\n",
    "     \n",
    "    # printing all tables list\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "11fad5f2-5769-4653-ba75-eea52a2c1708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of tables\n",
      "\n",
      "[('Tell me a poem where 6th grade student can understand?', \"[('_type', 'openai'), ('frequency_penalty', 0), ('logit_bias', {}), ('max_tokens', 256), ('model_name', 'gpt-3.5-turbo-instruct'), ('n', 1), ('presence_penalty', 0), ('stop', None), ('temperature', 0.7), ('top_p', 1)]\", 0, '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"output\", \"Generation\"], \"kwargs\": {\"text\": \"\\\\n\\\\nThe world is a classroom,\\\\nWhere we learn and grow,\\\\nWith each lesson we\\'re given,\\\\nWe continue to glow.\\\\n\\\\nWe read and we write,\\\\nWe add and we subtract,\\\\nWe learn to be kind,\\\\nAnd to always act.\\\\n\\\\nWe make mistakes,\\\\nBut that\\'s okay,\\\\nFor we\\'re still learning,\\\\nEvery single day.\\\\n\\\\nWe dream big dreams,\\\\nAnd set our goals high,\\\\nFor we know with hard work,\\\\nWe can reach for the sky.\\\\n\\\\nSo let\\'s embrace this journey,\\\\nAs 6th graders we\\'ll thrive,\\\\nFor the world is our classroom,\\\\nAnd learning is our drive.\", \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"Generation\"}}'), ('Tell me a joke where 6th grade student can understand?', \"[('_type', 'openai'), ('frequency_penalty', 0), ('logit_bias', {}), ('max_tokens', 256), ('model_name', 'gpt-3.5-turbo-instruct'), ('n', 1), ('presence_penalty', 0), ('stop', None), ('temperature', 0.7), ('top_p', 1)]\", 0, '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"output\", \"Generation\"], \"kwargs\": {\"text\": \"\\\\n\\\\nWhy was the math book sad?\\\\n\\\\nBecause it had too many problems.\", \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"Generation\"}}'), ('Tell me a joke ?', \"[('_type', 'openai'), ('frequency_penalty', 0), ('logit_bias', {}), ('max_tokens', 256), ('model_name', 'gpt-3.5-turbo-instruct'), ('n', 1), ('presence_penalty', 0), ('stop', None), ('temperature', 0.7), ('top_p', 1)]\", 0, '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"output\", \"Generation\"], \"kwargs\": {\"text\": \"\\\\n\\\\nWhy couldn\\'t the bicycle stand up by itself?\\\\n\\\\nBecause it was two-tired!\", \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"Generation\"}}'), ('Tell a joke ?', \"[('_type', 'openai'), ('frequency_penalty', 0), ('logit_bias', {}), ('max_tokens', 256), ('model_name', 'gpt-3.5-turbo-instruct'), ('n', 1), ('presence_penalty', 0), ('stop', None), ('temperature', 0.7), ('top_p', 1)]\", 0, '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"output\", \"Generation\"], \"kwargs\": {\"text\": \"\\\\n\\\\nWhy don\\'t scientists trust atoms?\\\\n\\\\nBecause they make up everything.\", \"generation_info\": {\"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"Generation\"}}')]\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\"SELECT * FROM full_llm_cache \"\"\"\n",
    " \n",
    "# Creating cursor object using connection object\n",
    "cursor = conn.cursor()\n",
    "     \n",
    "    # executing our sql query\n",
    "cursor.execute(sql_query)\n",
    "print(\"List of tables\\n\")\n",
    "     \n",
    "    # printing all tables list\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31624edc-e0d3-4074-9cf4-ed67f6a3554f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d47be202-43af-4b45-bbf1-0166c97fe37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "19b76e70-22c4-42d4-ae9b-a2f8088dfc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Noordeen! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 15, 'total_tokens': 27}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-574226af-178c-4b28-890e-54011b9960cf-0')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi my name is noordeen?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "aac98ab7-9dde-47a1-9596-963782fec815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, I do not have access to personal information such as your name.\", response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 12, 'total_tokens': 29}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5c1f2aa9-d0ec-4be3-bdfb-f270d274158f-0')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4dbef1c9-6fe6-4b4c-9241-34237dd914ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-3.5-turbo-instruct\")   ## New Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "62055547-9bfe-4885-a128-6a76256854dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:158\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    155\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    157\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 158\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    168\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:560\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    554\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    558\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    559\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:421\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    420\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 421\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    422\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    423\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    425\u001b[0m ]\n\u001b[1;32m    426\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:411\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 411\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m         )\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:632\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:522\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    521\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 522\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/resources/chat/completions.py:606\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    605\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1028\u001b[0m )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}"
     ]
    }
   ],
   "source": [
    "llm.invoke('HI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "75ec3ba7-45aa-4998-ab22-99b62404f62f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/llms.py:276\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 276\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    288\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/llms.py:819\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    808\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    809\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    810\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[1;32m    818\u001b[0m     ]\n\u001b[0;32m--> 819\u001b[0m     new_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m     llm_output \u001b[38;5;241m=\u001b[39m update_cache(\n\u001b[1;32m    823\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache,\n\u001b[1;32m    824\u001b[0m         existing_prompts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m         prompts,\n\u001b[1;32m    829\u001b[0m     )\n\u001b[1;32m    830\u001b[0m     run_info \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    831\u001b[0m         [RunInfo(run_id\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mrun_id) \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers]\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    834\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 657\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/langchain_openai/llms/base.py:350\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     choices\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    335\u001b[0m         {\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m         }\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[1;32m    354\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/resources/completions.py:528\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    527\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m Stream[Completion]:\n\u001b[0;32m--> 528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_of\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mecho\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/openai/_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1028\u001b[0m )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model = 'gpt-3.5-turbo')  ## Legacy\n",
    "llm.invoke('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd160a9-b6f8-44ff-a2a3-97cf8afa9eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c0b27-a1fa-4040-83e8-90f63fe49972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cb24facc-fdfc-4b9a-a2c8-b3b87057cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-4\")   ## New Chat Model\n",
    "prompt = 'Write an essay on pollution, stress the situation on Air polution in Delhi, Add more statistics information. Concentrate on Health issue and Give the suggestion to overcome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c36a222f-594b-4fd3-bfe1-ff9f0897baa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Air Pollution in Delhi: A Public Health Crisis\n",
      "\n",
      "Pollution, in its various forms, has become a global concern that poses serious threats to the environment and human health. Among the different types of pollution, air pollution has drawn significant attention due to its direct and immediate impact on human well-being. Delhi, the capital city of India, has become an unfortunate emblem of this crisis, with air pollution levels regularly surpassing safe limits.\n",
      "\n",
      "Delhi, with a population of over 30 million, has been grappling with severe air pollution for several years. According to a 2019 report by IQAir AirVisual, Delhi was ranked as the most polluted capital city in the world. The city's Air Quality Index (AQI) consistently falls under the 'severe' or 'hazardous' category, particularly during the winter months. The World Health Organization (WHO) considers an AQI above 25 as unsafe, but Delhi's AQI often exceeds 300, and at times, it has even crossed 1000.\n",
      "\n",
      "The primary sources of air pollution in Delhi include vehicle emissions, industrial activities, waste burning, and construction work. Moreover, seasonal factors like crop burning in neighbouring states and firecracker burning during festivals exacerbate the situation. This deadly cocktail of pollutants not only obscures Delhi's skies but also infiltrates the lungs of its residents, leading to a plethora of health issues.\n",
      "\n",
      "Air pollution has been linked to numerous health problems ranging from minor irritations like coughing and throat irritation to severe conditions like asthma, heart diseases, and even lung cancer. A 2020 study by Lancet Planetary Health estimated that air pollution contributed to more than 16.7 lakh deaths in India in 2019, with Delhi being a major contributor. Furthermore, children exposed to polluted air are more likely to develop respiratory problems, impaired lung growth, and cognitive issues.\n",
      "\n",
      "Addressing this public health crisis requires a comprehensive and multi-pronged approach. Firstly, stricter emission norms for vehicles and industries need to be implemented and enforced. The transition to cleaner fuels and renewable energy sources should be encouraged. Public transportation and non-motorized transport systems like cycling and walking should be promoted to reduce vehicular pollution.\n",
      "\n",
      "Secondly, waste management practices need to be improved to reduce waste burning. The government should also invest in modern technology for waste disposal and recycling. Moreover, the practice of crop burning should be discouraged through awareness campaigns, and farmers should be provided with alternative methods for crop residue management.\n",
      "\n",
      "Thirdly, public awareness about the dangers of air pollution should be raised. People should be encouraged to reduce their pollution footprint by adopting greener lifestyle choices. Regular health check-ups should be promoted, especially for those living in high-risk areas.\n",
      "\n",
      "Lastly, but importantly, there is a need for stronger laws and regulations to control air pollution and stricter penalties for violations. The government should also invest in continuous monitoring and research to understand the impact of air pollution on public health better.\n",
      "\n",
      "In conclusion, air pollution in Delhi is not just an environmental issue but a major public health crisis that requires immediate and sustained intervention. The city's air pollution problem is a stark reminder of the urgent need for a shift towards sustainable development and a cleaner, healthier future for all.\n",
      "CPU times: user 15.2 ms, sys: 4.55 ms, total: 19.7 ms\n",
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3d64a941-7178-4627-8e3f-1d106fa6ab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Air Pollution Crisis in Delhi: A Health Emergency and Possible Solutions\n",
      "\n",
      "Pollution, a global menace, has become a severe and pressing issue in recent years. Among the various types of pollution, air pollution poses a significant threat to the environment and human health. Often described as an 'invisible killer,' air pollution is responsible for an estimated seven million premature deaths annually, according to the World Health Organization (WHO). However, the situation is particularly alarming in Delhi, the capital city of India. \n",
      "\n",
      "Delhi, one of the most populous cities in the world, has been grappling with catastrophic levels of air pollution for several years. The Central Pollution Control Board (CPCB) reports that Delhi's air quality index (AQI) oscillates between 'poor' and 'severe' for the majority of the year, with the situation worsening during the winter months. In November 2020, the AQI in Delhi soared past 400, pushing it into the 'severe' category. To put things into perspective, an AQI above 100 is considered unhealthy by the United States Environmental Protection Agency.\n",
      "\n",
      "The dire air quality in Delhi is primarily attributed to vehicular emissions, industrial pollution, construction dust, and crop residue burning in neighboring states. Moreover, the city's geographical location and weather conditions exacerbate the pollution levels, trapping harmful pollutants in the atmosphere and creating a toxic smog that blankets the city.\n",
      "\n",
      "The health consequences of Delhi's air pollution are alarming. The Lancet Commission on Pollution and Health indicates that pollution is the largest environmental cause of disease and premature death in the world today. Prolonged exposure to polluted air can lead to a host of health problems such as respiratory diseases, cardiovascular conditions, strokes, and lung cancer. Children and the elderly are particularly vulnerable, with many developing asthma and other respiratory conditions. A study by the University of Chicago indicates that residents of Delhi could live up to nine years longer if the city met the WHO air quality standards.\n",
      "\n",
      "In response to this public health emergency, urgent and concerted action is required at various levels. To begin with, stricter enforcement of environmental regulations is crucial. This includes stringent emission norms for industries and vehicles, proper waste management practices, and a ban on crop residue burning. \n",
      "\n",
      "Moreover, there's a need to promote clean and renewable energy alternatives. The government should incentivize the use of electric vehicles and solar power to reduce dependence on fossil fuels. Public transportation should also be enhanced and encouraged to minimize vehicular pollution.\n",
      "\n",
      "Additionally, citizens can play a pivotal role in combating air pollution. Simple practices such as carpooling, minimizing the use of private vehicles, and planting trees can contribute significantly towards improving air quality. \n",
      "\n",
      "In conclusion, the air pollution crisis in Delhi is a glaring reminder of the interplay between human activities and environmental health. While the situation is grim, it is not insurmountable. A collective commitment to environmental sustainability, stringent policy measures, and individual responsibility can pave the way towards cleaner and healthier air. The time for action is now; our health and the health of future generations depend on it.CPU times: user 831 ms, sys: 173 ms, total: 1 s\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content,end = '', flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90edc293-85a4-4318-9939-734d49bb2e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "i/p -> How are you?\n",
    "\n",
    "o/p -> i am fine <EOS>\n",
    "6 tokens\n",
    "\n",
    "\n",
    "Decoder Architecture?\n",
    "\n",
    "Predict the next token till it reaches <EOS> or till it reaches Max token \n",
    "\n",
    "for each token to predict the model takes 1 sec -> 100 tokens -> 100 sec -> \n",
    "wait for 100 sec to get he response\n",
    "\n",
    "\n",
    "Once LLM predicts one token give me the response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10df09d1-2672-48b9-acea-0cc34ad7341d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c894078-e9c2-4478-8b37-a1f19885adb0",
   "metadata": {},
   "source": [
    "# PromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a9037ec8-fc01-4cd6-84ff-f17531931106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b0df1d52-0f74-46f0-8e19-e89ac33fa9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" you are an experienced physics teacher.\n",
    "write a few sentence about the following concept {concept} in {language}. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "13e60c7d-8211-42cd-857d-dc7eded14f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "promptTemplate = PromptTemplate.from_template(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2f93f5da-c621-4dd1-82f9-d64a029df7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = promptTemplate.format(concept = 'Thermodynamics',language = 'Hinglish' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9ec9ed7f-6e03-43d1-a711-eac89da136e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model= 'gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6c7ac890-e66e-49cb-b318-74d6c65d95f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thermodynamics ek branch hai physics ka jo energy, heat aur temperature ke interaction par focus karta hai. Yeh study karne mein help karta hai ki energy kaise different forms mein convert hoti hai, jaise ki heat se mechanical energy mein. Yeh bhi samjhaata hai kaise energy conservation work karta hai. Thermodynamics ke teen main laws hote hain jo explain karte hain energy ki conservation, entropy aur absolute zero concept ko.'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6fbf2183-33ed-4e92-9a41-f9b235a213a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept = ['Thermodynamics','Gravity','Newtons Law', 'Law of attraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a8a2cdc2-282d-4e0a-83f3-973f52b1b364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCEPT -  Thermodynamics\n",
      "Thermodynamics ek physics ka branch hai jo energy, heat aur work ke concepts ko study karta hai. Ye batata hai ki energy kaise convert hoti hai ek form se doosre form mein aur iske laws hame batate hain ki energy kabhi bhi create ya destroy nahi ho sakti, sirf transform hoti hai. Isme mainly 4 laws hote hain jo systems ki thermal energy ke behavior ko describe karte hain.-----------\n",
      "CONCEPT -  Gravity\n",
      "Gravity ek aisi shakti hai jo do padartho ko ek dusre ki taraf khichti hai. Dharti par humein iss shakti ka asar sabse zyada mehsus hota hai. Jab hum koi cheez upar fekte hain, woh neeche gir jaati hai, yeh gravity ki wajah se hota hai. Issi gravity ki wajah se hum dharti par tikte hain aur humara soorya, chand aur anya graho ke saath sambandh bana rehta hai.-----------\n",
      "CONCEPT -  Newtons Law\n",
      "Newton ke teen laws hote hain jo physics mein bahut important role play karte hain. Pehla law, yaani ki inertia ka law kehta hai ki koi bhi object apne aap se rest ya motion mein nahi aayega jab tak ki koi bahari force uspe act na kare. Dusra law, yaani ki F=ma, yeh batata hai ki ek object par lage force ka effect uski acceleration par hota hai. Teesra law action-reaction ka law hai, jo kehta hai ki har action ka equal aur opposite reaction hota hai.-----------\n",
      "CONCEPT -  Law of attraction\n",
      "Law of Attraction ek aisi universal principle hai jo suggest karti hai ki humare thoughts aur beliefs humare reality ko shape karte hain. Yadi hum positive sochte hain, toh positive outcomes aate hain, aur agar hum negative sochte hain, toh negative results aate hain. Ye ek tarah ka 'like attracts like' principle hai, jiska use hum apne life goals aur dreams ko achieve karne ke liye kar sakte hain.-----------\n"
     ]
    }
   ],
   "source": [
    "for cpt in concept:\n",
    "    print(\"CONCEPT - \",cpt)\n",
    "    prompt = promptTemplate.format(concept = cpt,language = 'Hinglish' )\n",
    "    for chunk in chat.stream(prompt):\n",
    "        print(chunk.content,end = '', flush = True)\n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a4899-50b6-458b-92ae-7ba442da5396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1621c9c3-ad93-4daa-8522-7a4b59f740bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Olivia\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "86cd45b5-e0c9-46cc-9ca2-dcdb4e3a6cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Olivia. How can I assist you today?'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt_value).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91d645-33d6-457a-a77e-3c3df58ea723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d399f69-75e9-46e4-b1d8-4408149c669a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e6537-bc99-4fc0-a9a3-950f2e9fa93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f097a504-bf42-4468-a6ea-bc367b7b234f",
   "metadata": {},
   "source": [
    "# Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7d9545f2-3114-4d3d-bc68-91fc0153b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "3fbd812f-1fba-45a2-ab7c-3f69ab7fe765",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm  = ChatOpenAI()\n",
    "\n",
    "template = \"\"\" you are an experienced physics teacher.\n",
    "write a few sentence about the following concept {concept} in {language}. \"\"\"\n",
    "\n",
    "promptTemplate = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt= promptTemplate, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "419572e4-4ddd-45a4-90f2-3ecb1b62df1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m you are an experienced physics teacher.\n",
      "write a few sentence about the following concept Thermodynamics in Hinglish. \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'concept': 'Thermodynamics',\n",
       " 'language': 'Hinglish',\n",
       " 'text': 'Thermodynamics ek bhaut important concept hai physics mein. Yeh study karta hai energy ke transfer aur transformation ke baare mein. Isme hum heat, work, aur energy ke interrelation ko samajhte hain. Yeh concept humein samjhaata hai ki kaise energy ek form se doosri form mein convert hoti hai.'}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'concept' : 'Thermodynamics','language' : 'Hinglish'} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "71263556-e2b1-4465-a6b8-c6d2b6a0e4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCEPT -  Thermodynamics\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the language you want to see the result French\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m you are an experienced physics teacher.\n",
      "write a few sentence about the following concept Thermodynamics in French. \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'concept': 'Thermodynamics', 'language': 'French', 'text': \"La thermodynamique est une branche de la physique qui étudie les transferts d'énergie sous forme de chaleur et de travail. Elle s'intéresse aux processus de transformation de l'énergie et à l'équilibre des systèmes thermiques. Les lois de la thermodynamique définissent les principes fondamentaux régissant ces phénomènes et sont essentielles pour comprendre le fonctionnement de nombreux systèmes naturels et artificiels.\"}-----------\n",
      "CONCEPT -  Gravity\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the language you want to see the result Tamil\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m you are an experienced physics teacher.\n",
      "write a few sentence about the following concept Gravity in Tamil. \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'concept': 'Gravity', 'language': 'Tamil', 'text': 'இரும்பு என்பது புலம்பலின் ஒரு முக்கியமான அச்சிலம் அல்லது அன்புள்ள படுக்கையை ஆக்கும் பின்னல் வலத்தையான பூர்வத்து அளவின்மையில் தூக்கியிருக்கும் இரும்பு அல்லது படுக்கையை விளக்கும் அல்லது அதிக ஒழுங்கின் மேலும் குறைவான உள்ளான் நீங்கள் அச்சமாக இரும்பு என்னும் உண்மை அறியுவீர்கள்.'}-----------\n",
      "CONCEPT -  Newtons Law\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the language you want to see the result Arabic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m you are an experienced physics teacher.\n",
      "write a few sentence about the following concept Newtons Law in Arabic. \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'concept': 'Newtons Law', 'language': 'Arabic', 'text': 'قانون نيوتن هو مبدأ في الفيزياء ينص على أن الجسم يظل في حالة سكون أو حركة مستقرة إلى أن يتأثر بتأثير خارجي يغير حالته. ويمكن تعبير عن هذا القانون بالصيغة F = ma حيث F هو القوة المؤثرة، m هو الكتلة، و a هو التسارع الناتج عن هذه القوة.'}-----------\n",
      "CONCEPT -  Law of attraction\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the language you want to see the result English\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m you are an experienced physics teacher.\n",
      "write a few sentence about the following concept Law of attraction in English. \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'concept': 'Law of attraction', 'language': 'English', 'text': \"The Law of Attraction is a belief that positive or negative thoughts bring positive or negative experiences into a person's life. It suggests that focusing on positive thoughts and intentions can attract positive outcomes and opportunities, while negative thoughts can attract negative outcomes. This concept is often associated with the power of visualization and manifestation in achieving one's goals and desires.\"}-----------\n"
     ]
    }
   ],
   "source": [
    "for cpt in concept:\n",
    "    print(\"CONCEPT - \",cpt)\n",
    "    lang = input('Enter the language you want to see the result')\n",
    "    dic = {'concept' : cpt,'language' : lang}\n",
    "    for chunk in chain.stream(dic):\n",
    "        print(chunk,end = '', flush = True)\n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67b72e-1928-4aa1-a6c5-f268590097ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9816cbe9-302d-44dd-93e2-115f215c5997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mSure! Here is an example of a Python function that implements linear regression:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def linear_regression(x, y):\n",
      "    n = len(x)\n",
      "    x_mean = np.mean(x)\n",
      "    y_mean = np.mean(y)\n",
      "\n",
      "    numerator = 0\n",
      "    denominator = 0\n",
      "    for i in range(n):\n",
      "        numerator += (x[i] - x_mean) * (y[i] - y_mean)\n",
      "        denominator += (x[i] - x_mean) ** 2\n",
      "\n",
      "    slope = numerator / denominator\n",
      "    intercept = y_mean - slope * x_mean\n",
      "\n",
      "    return slope, intercept\n",
      "\n",
      "# Example usage\n",
      "x = np.array([1, 2, 3, 4, 5])\n",
      "y = np.array([2, 3, 4, 5, 6])\n",
      "\n",
      "slope, intercept = linear_regression(x, y)\n",
      "print(\"Slope:\", slope)\n",
      "print(\"Intercept:\", intercept)\n",
      "```\n",
      "\n",
      "This function calculates the slope and intercept of the line that best fits the given data points using the method of linear regression. It takes two arrays `x` and `y` as input, representing the x and y coordinates of the data points. The function then calculates the slope and intercept of the regression line and returns them as output.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe provided Python function `linear_regression` is designed to perform a simple linear regression analysis to find the best-fitting linear relationship between a set of paired data points. Linear regression aims to model the relationship between two variables by fitting a linear equation (`y = mx + b`) to observable data. One variable is considered independent (x), and the other dependent (y).\n",
      "\n",
      "Here's a breakdown of the function:\n",
      "\n",
      "### Importing Necessary Libraries\n",
      "- `import numpy as np`: This line imports the numpy library, which provides support for arrays, mathematical operations, and many convenient mathematical functions. In this function, `numpy` is used for calculating mean values of arrays.\n",
      "\n",
      "### Function Definition\n",
      "- `def linear_regression(x, y)`: Defines the function named `linear_regression` that takes two parameters, `x` and `y`. These parameters are expected to be 'numpy arrays' or lists containing the x and y coordinates of data points, respectively.\n",
      "\n",
      "### Preparing for Calculations\n",
      "- `n = len(x)`: Determines the number of data points by calculating the length of the `x` array.\n",
      "- `x_mean = np.mean(x)`, `y_mean = np.mean(y)`: Calculates the mean (average) values of the `x` and `y` datasets, which will be used in computing the slope and intercept.\n",
      "\n",
      "### Calculating Slope and Intercept\n",
      "The core of the linear regression analysis involves computing the slope (`m`) and intercept (`b`). \n",
      "- `numerator` and `denominator` are initialized to 0. These variables will be used to accumulate the sums required for the slope calculation.\n",
      "- A for loop iterates through each index from `0` to `n-1`, performing calculations on every pair `(x[i], y[i])`:\n",
      "  - The `numerator` sums up the products of the difference between each x value and the mean of x, and the difference between each y value and the mean of y. This measures the covariance of x and y.\n",
      "  - The `denominator` accumulates the squares of the differences between each x value and the mean of x, calculating the variance of x.\n",
      "- The slope (`m`) is the ratio of `numerator` to `denominator`. This slope represents the change in y for a one-unit change in x.\n",
      "- The intercept (`b`) is computed based on the mean values of x and y, less the product of the slope and the mean x value. It represents the point at which the regression line crosses the y-axis.\n",
      "\n",
      "### Returning Results\n",
      "- The function returns two values: `slope` and `intercept`, which together define the best-fitting line's equation `y = slope * x + intercept` for the data points.\n",
      "\n",
      "### Example Usage\n",
      "The example:\n",
      "```python\n",
      "x = np.array([1, 2, 3, 4, 5])\n",
      "y = np.array([2, 3, 4, 5, 6])\n",
      "```\n",
      "creates numpy arrays `x` and `y` representing the data points. Invoking `linear_regression(x, y)` with these arrays calls the function to compute and print the slope and intercept of the best-fitting line.\n",
      "\n",
      "### Print Output\n",
      "- The printed output confirmations in the form of:\n",
      "```python\n",
      "Slope: 1.0\n",
      "Intercept: 1.0\n",
      "```\n",
      "indicate that the best-fitting line increases by 1 unit in `y` for each increase of 1 unit in `x`, and crosses the Y-axis at 1. This illustrates a perfect linear relationship with no errors between the variables, making the best-fitting line indeed very \"fit\" for the given data points.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# Initialize the first ChatOpenAI model (gpt-3.5-turbo) with specific temperature\n",
    "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "\n",
    "# Define the first prompt template\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template='You are an experienced scientist and Python programmer. Write a function that implements the concept of {concept}.'\n",
    ")\n",
    "# Create an LLMChain using the first model and the prompt template\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "# Initialize the second ChatOpenAI model (gpt-4-turbo) with specific temperature\n",
    "llm2 = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=1.2)\n",
    "\n",
    "# Define the second prompt template\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
    ")\n",
    "# Create another LLMChain using the second model and the prompt template\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)\n",
    "\n",
    "# Combine both chains into a SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "# Invoke the overall chain with the concept \"linear regression\"\n",
    "output = overall_chain.invoke('linear regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "03a09845-18b5-42f2-8ecb-049a468998d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm  = ChatOpenAI(model = 'gpt-3.5-turbo-16k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "c2112e95-be10-406d-a760-e9c347c55d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='To find the square root of the factorial of 11, we first need to calculate the factorial of 11, which is \\\\(11!\\\\). The factorial of a number \\\\(n\\\\), denoted by \\\\(n!\\\\), is the product of all positive integers less than or equal to \\\\(n\\\\).\\n\\nSo, \\\\(11! = 11 \\\\times 10 \\\\times 9 \\\\times 8 \\\\times 7 \\\\times 6 \\\\times 5 \\\\times 4 \\\\times 3 \\\\times 2 \\\\times 1\\\\).\\n\\nCalculating this gives:\\n\\n\\\\(11! = 39916800\\\\)\\n\\nNow, we need to find the square root of \\\\(39916800\\\\).\\n\\n\\\\(\\\\sqrt{39916800} \\\\approx 6319.8079\\\\)\\n\\nTherefore, the square root of the factorial of 11, displayed with 4 decimal points, is approximately \\\\(6319.8079\\\\).', response_metadata={'token_usage': {'completion_tokens': 189, 'prompt_tokens': 25, 'total_tokens': 214}, 'model_name': 'gpt-4-turbo-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-fb39229a-55da-483e-aa85-034aa0ea0307-0')"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = 'Calculate the square root of the factorial of 11 and display it with 4 decimal points'\n",
    "\n",
    "llm.invoke(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3021501b-9674-450a-a2af-88b536432ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo solve this, I will first calculate the factorial of 11 using the math library in Python. Then, I will calculate the square root of that result. Finally, I will format the output to display it with 4 decimal points. Let's start by importing the math library and calculating the factorial of 11.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: import math\n",
      "print(math.factorial(11))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m39916800\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that I have the factorial of 11, I will calculate the square root of this number and format the output to display it with 4 decimal points.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: print(f\"{math.sqrt(39916800):.4f}\")\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m6317.9744\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "\n",
      "Final Answer: 6317.9744\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the square root of the factorial of 11 and display it with 4 decimal points',\n",
       " 'output': '6317.9744'}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the ChatOpenAI model with gpt-4-turbo and a temperature of 0\n",
    "llm = ChatOpenAI(model='gpt-4-turbo-preview', temperature=0)\n",
    "\n",
    "# Create a Python agent using the ChatOpenAI model and a PythonREPLTool\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Invoke the agent\n",
    "prompt = 'Calculate the square root of the factorial of 11 and display it with 4 decimal points'\n",
    "agent_executor.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79802f4-ff4a-46ef-b89d-b4b13cda9707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22e759-0b9f-4691-83c9-1ff75a3790ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Who is the cheif minister of Andhara Pradesh?'\n",
    "\n",
    "llm.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5450bccd-0136-4201-9fdd-a54251181783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8150a06-3343-425e-8841-da71cbcf625e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1dbbe3-efa5-41f8-b002-9d7fcfbb4db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "deddea09-6870-4c5d-b857-05ed7650bc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from langchain-huggingface)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in /opt/homebrew/lib/python3.11/site-packages (from langchain-huggingface) (0.1.52)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tokenizers>=0.19.1 (from langchain-huggingface)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting transformers>=4.39.0 (from langchain-huggingface)\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.1.81)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/homebrew/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.5.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (8.2.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.1.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.25.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.11.4)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (9.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers>=4.39.0->langchain-huggingface) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/homebrew/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-huggingface) (3.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.14.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "Downloading langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\n",
      "Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers, sentence-transformers, langchain-huggingface\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.2\n",
      "    Uninstalling huggingface-hub-0.20.2:\n",
      "      Successfully uninstalled huggingface-hub-0.20.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.16.1 requires fsspec[http]<=2023.10.0,>=2023.1.0, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.23.4 langchain-huggingface-0.0.3 sentence-transformers-3.0.1 tokenizers-0.19.1 transformers-4.41.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "33f8b81d-f75b-4000-bce6-4c61307bbea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import  huggingface_hub\n",
    "huggingface_hub.login('hf_****************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "94adc644-79bf-4fd1-aebf-8353160b45eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint,ChatHuggingFace\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5cdb4328-4f13-4748-9ee9-fd696a67a507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' The sentence \"Hi, how are you?\" in Tamil is \"Hi, unakku solringa?\" According to the formal rules of Tamil grammar, the word order of the sentence should be \"Unakku solringa, hi?\" However, in informal spoken Tamil, it is common to use the English word order, resulting in \"Hi, unakku solringa?\" which is more widely understood and used.', response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=92, prompt_tokens=29, total_tokens=121), 'model': '', 'finish_reason': 'eos_token'}, id='run-bb2eabd2-d5b5-4917-858c-a5dd8c1e8af0-0')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to Tamil.\"),\n",
    "    (\"human\", \"Hi How are you?\")]\n",
    "\n",
    "chat.invoke(\"You are a helpful translator. Translate the user sentence to Tamil. Hi How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ce0acb07-09c6-4e7f-83c2-da14a368f1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Title: Addressing Youth Unemployment and Health Issues in Chennai: A Call to Action\\n\\nIntroduction\\n\\nChennai, often referred to as the cultural capital of India, is a vibrant city that has given rise to many innovations, advancements, and remarkable personalities. However, despite its booming economy and industrial growth, the city continues to grapple with a perplexing problem: youth unemployment. This essay explores' response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=100, prompt_tokens=45, total_tokens=145), 'model': '', 'finish_reason': 'length'} id='run-d183349b-2de7-4b83-97ea-72d7a71475f5-0'\n",
      "CPU times: user 8.57 ms, sys: 2.93 ms, total: 11.5 ms\n",
      "Wall time: 2.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = 'Write an essay on Unemployement, stress the situation on youth Unemployement in Chennai, Add more statistics information. Concentrate on Health issue and Give the suggestion to overcome'\n",
    "for chunk in chat.stream(prompt):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a73d8117-d22c-489d-b6cd-99342b7c9832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Title: Addressing Youth Unemployment and Health Issues in Chennai: A Call to Action\\n\\nIntroduction\\n\\nChennai, often referred to as the cultural capital of India, is a vibrant city that has given rise to many innovations, advancements, and remarkable personalities. However, despite its booming economy and industrial growth, the city continues to grapple with a perplexing problem: youth unemployment. This essay explores', response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 45, 'total_tokens': 145}, 'model': '', 'finish_reason': 'length'}, id='run-d183349b-2de7-4b83-97ea-72d7a71475f5-0')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e5ae9e-4248-4d3c-b761-efe8773d5301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
